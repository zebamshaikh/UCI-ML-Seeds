{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e110834e11fcf5500f4dd9e5f72b5b8",
     "grade": false,
     "grade_id": "cell-80b77fe0d5d362ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Explain the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3ae4a44207d40f055e076aabbb09b45",
     "grade": true,
     "grade_id": "cell-e573d055cfb4a916",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The “tradeoff” between bias and variance can be viewed in this manner – a learning algorithm with low bias must be “flexible” so that it can fit the data well. But if the learning algorithm is too flexible (for instance, too linear), it will fit each training data set differently, and hence have high variance. A key characteristic of many supervised learning methods is a built-in way to control the bias-variance tradeoff either automatically or by providing a special parameter that the data scientist can adjust.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "83886665b6d5458966acb332c5e17c68",
     "grade": false,
     "grade_id": "cell-c40be3d158eb9ad1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Discuss the pros and cons of using the BIC to select a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "25405a06bb3615c01ebf2da1294c49a8",
     "grade": true,
     "grade_id": "cell-5df930a8f675836a",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Pros:\n",
    "-BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.\n",
    "\n",
    "Cons:\n",
    "\n",
    "-BIC is only valid for sample size n much larger than the number of parameters k in the model.\n",
    "-BIC cannot handle complex collections of models as in the feature selection problem in high-dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection on a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data = read.csv(\"data/iris.csv\", row.names='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\", \n",
       "    data = iris.data)\n",
       "\n",
       "Deviance Residuals: \n",
       "     Min        1Q    Median        3Q       Max  \n",
       "-0.59046  -0.15230   0.01338   0.10332   0.55061  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)   0.19208    0.20470   0.938 0.349611    \n",
       "sepal_length -0.10974    0.05776  -1.900 0.059418 .  \n",
       "sepal_width  -0.04424    0.05996  -0.738 0.461832    \n",
       "petal_length  0.22700    0.05699   3.983 0.000107 ***\n",
       "petal_width   0.60989    0.09447   6.456 1.52e-09 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for gaussian family taken to be 0.04798457)\n",
       "\n",
       "    Null deviance: 100.0000  on 149  degrees of freedom\n",
       "Residual deviance:   6.9578  on 145  degrees of freedom\n",
       "AIC: -22.935\n",
       "\n",
       "Number of Fisher Scoring iterations: 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris.glm = glm(\"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\", data = iris.data)\n",
    "summary(iris.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Log-Likelihood\n",
    "\n",
    "Without going too far into the math, we can think of the log-likelihood as a **likelihood function** telling us how likely a model is given the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is not human interpretable but is useful as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' 17.46751 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logLik(iris.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"All models are wrong, but some are useful.\" - George Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be concerned with one additional property - the **complexity** of the model. \n",
    "\n",
    "##### William of Occam\n",
    "\n",
    "[**Occam's razor**](https://en.wikipedia.org/wiki/Occam's_razor) is the problem-solving principle that, when presented with competing hypothetical answers to a problem, one should select the one that makes the fewest assumptions.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ab/William_of_Ockham_-_Logica_1341.jpg\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent this idea of complexity in terms of both the number of features we use and the amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
    "\n",
    "The BIC is formally defined as\n",
    "\n",
    "$$ \\mathrm{BIC} = {\\ln(n)k - 2\\ln({\\widehat L})}. $$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\widehat L$ = the maximized value of the likelihood function of the model $M$\n",
    "- $x$ = the observed data\n",
    "- $n$ = the number of data points in $x$, the number of observations, or equivalently, the sample size;\n",
    "- $k$ = the number of parameters estimated by the model. For example, in multiple linear regression, the estimated parameters are the intercept, the $q$ slope parameters, and the constant variance of the errors; thus, $k = q + 2$.\n",
    "\n",
    "\n",
    "It might help us to think of it as \n",
    "\n",
    "$$ \\mathrm{BIC} = \\text{complexity}-\\text{likelihood}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-4.87121487462612"
      ],
      "text/latex": [
       "-4.87121487462612"
      ],
      "text/markdown": [
       "-4.87121487462612"
      ],
      "text/plain": [
       "[1] -4.871215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BIC(iris.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' -4.871215 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = length(iris.glm$fitted.values)\n",
    "p = length(coefficients(iris.glm))\n",
    "\n",
    "likelihood = 2 * logLik(iris.glm)\n",
    "complexity = log(n)*(p+1)\n",
    "\n",
    "bic = complexity - likelihood\n",
    "bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIC_of_model = function (model) {\n",
    "    n = length(model$fitted.values)\n",
    "    p = length(coefficients(model))\n",
    "\n",
    "    likelihood = 2 * logLik(model)\n",
    "    complexity = log(n)*(p+1)\n",
    "\n",
    "    bic = complexity - likelihood\n",
    "    return(bic)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' -4.871215 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BIC_of_model(iris.glm) #df below means degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Here, we choose the optimal model by removing features one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1  = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\"\n",
    "model_2a = \"label ~ 1 + sepal_length + sepal_width + petal_length\"\n",
    "model_2b = \"label ~ 1 + sepal_length + sepal_width                + petal_width\"\n",
    "model_2c = \"label ~ 1 + sepal_length               + petal_length + petal_width\"\n",
    "model_2d = \"label ~ 1                + sepal_width + petal_length + petal_width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.glm.1 = glm(model_1, data=iris.data)\n",
    "iris.glm.2a = glm(model_2a, data=iris.data)\n",
    "iris.glm.2b = glm(model_2b, data=iris.data)\n",
    "iris.glm.2c = glm(model_2c, data=iris.data)\n",
    "iris.glm.2d = glm(model_2d, data=iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2a\"         \"28.0137935908893\"\n",
      "[1] \"model_2b\"         \"5.69337438932066\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_2d\"         \"-6.1930960954627\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC_of_model(iris.glm.1)))\n",
    "print(c('model_2a', BIC_of_model(iris.glm.2a )))\n",
    "print(c('model_2b', BIC_of_model(iris.glm.2b )))\n",
    "print(c('model_2c', BIC_of_model(iris.glm.2c )))\n",
    "print(c('model_2d', BIC_of_model(iris.glm.2d )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2a\"         \"28.0137935908893\"\n",
      "[1] \"model_2b\"         \"5.69337438932066\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_2d\"         \"-6.1930960954627\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC(iris.glm.1)))\n",
    "print(c('model_2a', BIC(iris.glm.2a )))\n",
    "print(c('model_2b', BIC(iris.glm.2b )))\n",
    "print(c('model_2c', BIC(iris.glm.2c )))\n",
    "print(c('model_2d', BIC(iris.glm.2d )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1  = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\"\n",
    "model_2c = \"label ~ 1 + sepal_length               + petal_length + petal_width\"\n",
    "model_3a = \"label ~ 1 + sepal_length               + petal_length \"\n",
    "model_3b = \"label ~ 1 + sepal_length                              + petal_width\"\n",
    "model_3c = \"label ~ 1                              + petal_length + petal_width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.glm.3a = glm(model_3a, data=iris.data)\n",
    "iris.glm.3b = glm(model_3b, data=iris.data)\n",
    "iris.glm.3c = glm(model_3c, data=iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_3a\"         \"25.3174210943167\"\n",
      "[1] \"model_3b\"         \"15.4504250116728\"\n",
      "[1] \"model_3c\"         \"-5.0467304546584\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC(iris.glm.1)))\n",
    "print(c('model_2c', BIC(iris.glm.2c )))\n",
    "print(c('model_3a', BIC(iris.glm.3a )))\n",
    "print(c('model_3b', BIC(iris.glm.3b )))\n",
    "print(c('model_3c', BIC(iris.glm.3c )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
